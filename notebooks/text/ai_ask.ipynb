{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AI_ASK\n",
    "\n",
    "## Overview\n",
    "This function interacts with an AI model to generate text-based responses based on a given prompt. It can optionally incorporate data provided as a 2D list into the prompt for more context-specific analysis or generation.\n",
    "\n",
    "## Usage\n",
    "To use the `AI_ASK` function in Excel, enter it as a formula in a cell, specifying your prompt and any optional arguments as needed:\n",
    "\n",
    "```excel\n",
    "=AI_ASK(prompt, [data], [temperature], [max_tokens], [model], [api_key], [api_url])\n",
    "```\n",
    "Replace each parameter with your desired value. The function returns a text response generated by the AI model.\n",
    "\n",
    "## Parameters\n",
    "| Parameter      | Type     | Required | Description                                                                                                 | Example |\n",
    "|---------------|----------|----------|-------------------------------------------------------------------------------------------------------------|---------|\n",
    "| prompt        | string   | Yes      | The question, task, or instruction for the AI.                                                              | \"Summarize the key findings from the employee engagement survey:\" |\n",
    "| data          | 2D list  | No       | Data from an Excel range to be included in the context sent to the AI.                                      | [[\"Question\", \"Score\"], [\"Team collaboration\", 4.5], ...] |\n",
    "| temperature   | float    | No       | Controls the randomness/creativity of the response (0.0 to 2.0). Higher values mean more creative.          | 0.5     |\n",
    "| max_tokens    | int      | No       | Maximum number of tokens (words/subwords) the AI should generate in its response (5 to 5000).               | 250     |\n",
    "| model         | string   | No       | The specific AI model ID to use for the request (e.g., 'mistral-small', 'mistral-large').                   | \"mistral-small-latest\" |\n",
    "| api_key       | string   | No       | API key for authentication. [Get a free API key from Mistral AI](https://console.mistral.ai/). | \"sk-...\" |\n",
    "| api_url       | string   | No       | OpenAI-compatible API endpoint URL which supports [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS). (e.g., https://api.mistral.ai/v1/chat/completions from Mistral AI). | \"https://api.mistral.ai/v1/chat/completions\" |\n",
    "\n",
    "## Return Value\n",
    "| Return Value | Type   | Description                                  | Example |\n",
    "|--------------|--------|----------------------------------------------|---------|\n",
    "| Response     | string | The text response generated by the AI model. | \"The survey indicates high satisfaction with team collaboration but highlights concerns about workload and career advancement opportunities.\" |\n",
    "\n",
    "## Demo\n",
    "If either `api_key` or `api_url` is not provided, both will default to Boardflare demo values (`api_url`: https://llm.boardflare.com, `api_key`: your Microsoft login token if available). This only works for users logged in with a Microsoft account and provides limited free demo usage. You may obtain a free api_key for [Mistral AI](https://console.mistral.ai/) with your Microsoft account which offers more generous free usage and supports CORS.\n",
    "\n",
    "## Limitations\n",
    "- The quality of the response depends on the clarity of the prompt and the data provided.\n",
    "- Large data ranges may exceed model context limits and result in truncated or incomplete responses.\n",
    "- The function requires an internet connection to access the AI model.\n",
    "- Model availability and output may vary depending on the provider or API changes.\n",
    "- Sensitive or confidential data should not be sent to external AI services.\n",
    "- `temperature` must be a float between 0 and 2 (inclusive). If not, an error message is returned.\n",
    "- `max_tokens` must be an integer between 5 and 5000 (inclusive). If not, an error message is returned.\n",
    "- If you hit the API rate limit for your provider, a message is returned instead of raising an exception.\n",
    "\n",
    "## Benefits\n",
    "- Automates text analysis, summarization, and business writing directly in Excel.\n",
    "- Saves time and improves consistency in reporting and communication.\n",
    "- Enables dynamic, context-aware responses using your own data.\n",
    "- More flexible and powerful than manual or native Excel approaches for text generation and analysis.\n",
    "\n",
    "## Examples\n",
    "\n",
    "### Employee Engagement Summary\n",
    "Ask for a summary of employee engagement survey results.\n",
    "\n",
    "**Sample Input Data (Range `A1:B5`):**\n",
    "\n",
    "| Question            | Score |\n",
    "|---------------------|-------|\n",
    "| Team collaboration  | 4.5   |\n",
    "| Workload            | 3.2   |\n",
    "| Career advancement  | 3.0   |\n",
    "| Management support  | 4.0   |\n",
    "\n",
    "```excel\n",
    "=AI_ASK(\"Summarize the key findings from the employee engagement survey:\", A1:B5)\n",
    "```\n",
    "**Sample Output:**\n",
    "\"The survey indicates high satisfaction with team collaboration but highlights concerns about workload and career advancement opportunities.\"\n",
    "\n",
    "### Quarterly Sales Analysis\n",
    "Analyze quarterly sales data and provide insights.\n",
    "\n",
    "**Sample Input Data (Range `A1:E4`):**\n",
    "\n",
    "| Region   | Q1   | Q2   | Q3   | Q4   |\n",
    "|----------|------|------|------|------|\n",
    "| North    | 120  | 135  | 150  | 160  |\n",
    "| South    | 100  | 110  | 120  | 130  |\n",
    "| Central  | 90   | 95   | 100  | 105  |\n",
    "\n",
    "```excel\n",
    "=AI_ASK(\"Provide a brief analysis of the quarterly sales performance:\", A1:E4)\n",
    "```\n",
    "**Sample Output:**\n",
    "\"Sales increased steadily across all regions, with the North region showing the highest growth in Q4.\"\n",
    "\n",
    "### Summarize Incident Report\n",
    "Summarize a block of operational incident text provided in a cell.\n",
    "\n",
    "**Sample Input Text (Cell `A1`):**\n",
    "\"On April 10th, a system outage affected order processing for 2 hours. The IT team resolved the issue by updating server configurations. No data loss occurred.\"\n",
    "\n",
    "```excel\n",
    "=AI_ASK(\"Summarize the following incident report in one sentence:\", A1)\n",
    "```\n",
    "**Sample Output:**\n",
    "\"A brief system outage on April 10th was quickly resolved by IT with no data loss.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def ai_ask(prompt, data=None, temperature=0.5, max_tokens=250, model='mistral-small-latest', api_key=None, api_url=\"https://api.mistral.ai/v1/chat/completions\"):\n",
    "    \"\"\"\n",
    "    Uses AI to generate responses based on prompts and optional data ranges.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The question, task, or analysis to perform\n",
    "        data (list, optional): 2D list containing data from Excel range to analyze\n",
    "        temperature (float, optional): Controls response creativity (0-2). Default is 0.5\n",
    "        max_tokens (int, optional): Maximum tokens for response generation\n",
    "        model (str, optional): ID of the model to use\n",
    "        api_key (str, optional): API key for authentication (e.g. for Mistral AI).  Get your own free Mistral API key at https://console.mistral.ai/\n",
    "        api_url (str, optional): OpenAI compatible URL. (e.g., https://api.mistral.ai/v1/chat/completions for Mistral AI).\n",
    "\n",
    "    Returns:\n",
    "        str: The AI-generated response\n",
    "    \"\"\"\n",
    "\n",
    "    # Login on the Functions tab for limited demo usage.  \n",
    "    if api_key is None or api_url is None:\n",
    "        if 'idToken' in globals():\n",
    "            api_key = globals()['idToken']\n",
    "            api_url = \"https://llm.boardflare.com\"\n",
    "        else:\n",
    "            return (\"Login on the Functions tab for limited demo usage, or sign up for a free Mistral AI account at https://console.mistral.ai/ and add your own api_key.\")\n",
    "\n",
    "    # Validate temperature\n",
    "    if not isinstance(temperature, (float, int)) or not (0 <= float(temperature) <= 2):\n",
    "        return \"Error: temperature must be a float between 0 and 2 (inclusive)\"\n",
    "    # Validate max_tokens\n",
    "    if not isinstance(max_tokens, int) or not (5 <= max_tokens <= 5000):\n",
    "        return \"Error: max_tokens must be an integer between 5 and 5000 (inclusive)\"\n",
    "\n",
    "    # Construct the message incorporating both prompt and data if provided\n",
    "    message = prompt\n",
    "    if data is not None:\n",
    "        data_str = json.dumps(data, indent=2)\n",
    "        message += f\"\\n\\nData to analyze:\\n{data_str}\"\n",
    "    \n",
    "    # Prepare the API request payload\n",
    "    payload = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": message}],\n",
    "        \"temperature\": temperature,\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Make the API request\n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "    if response.status_code == 429:\n",
    "        return \"You have hit the rate limit for the API. Please try again later.\"\n",
    "    response.raise_for_status()\n",
    "    # Extract and return the response content\n",
    "    response_data = response.json()\n",
    "    content = response_data[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "ipytest.autoconfig()\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent / \"test\"))\n",
    "from test_utils import get_graph_token\n",
    "\n",
    "def inject_id_token():\n",
    "    # Acquire token using shared utility\n",
    "    token = get_graph_token()\n",
    "    globals()[\"idToken\"] = token\n",
    "\n",
    "inject_id_token()\n",
    "\n",
    "def test_employee_engagement_summary():\n",
    "    prompt = \"Summarize the key findings from the employee engagement survey in 1 sentence:\"\n",
    "    data = [\n",
    "        [\"Question\", \"Score\"],\n",
    "        [\"Team collaboration\", 4.5],\n",
    "        [\"Workload\", 3.2],\n",
    "        [\"Career advancement\", 3.0],\n",
    "        [\"Management support\", 4.0]\n",
    "    ]\n",
    "    result = ai_ask(prompt, data=data)\n",
    "    assert isinstance(result, str)\n",
    "    assert any(word in result for word in [\"collaboration\", \"workload\", \"career\"])\n",
    "\n",
    "def test_quarterly_sales_analysis():\n",
    "    prompt = \"Provide a brief analysis of the quarterly sales performance in 1 sentence:\"\n",
    "    data = [\n",
    "        [\"Region\", \"Q1\", \"Q2\", \"Q3\", \"Q4\"],\n",
    "        [\"North\", 120, 135, 150, 160],\n",
    "        [\"South\", 100, 110, 120, 130],\n",
    "        [\"Central\", 90, 95, 100, 105]\n",
    "    ]\n",
    "    result = ai_ask(prompt, data=data)\n",
    "    assert isinstance(result, str)\n",
    "    assert any(word in result for word in [\"North\", \"growth\", \"sales\"])\n",
    "\n",
    "def test_operations_incident_summary():\n",
    "    prompt = \"Summarize the following incident report in 1 sentence:\"\n",
    "    data = [\n",
    "        [\"On April 10th, a system outage affected order processing for 2 hours. The IT team resolved the issue by updating server configurations. No data loss occurred.\"]\n",
    "    ]\n",
    "    result = ai_ask(prompt, data=data)\n",
    "    assert isinstance(result, str)\n",
    "    assert any(word in result for word in [\"outage\", \"resolved\", \"data loss\"])\n",
    "\n",
    "def test_invalid_temperature():\n",
    "    prompt = \"Test invalid temperature\"\n",
    "    data = [[\"Question\", \"Score\"], [\"Test\", 1.0]]\n",
    "    result = ai_ask(prompt, data=data, temperature=3.0)\n",
    "    assert isinstance(result, str)\n",
    "    assert result.startswith(\"Error: temperature\")\n",
    "\n",
    "ipytest.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio Demo\n",
    "import gradio as gr\n",
    "\n",
    "def run_ai_ask(prompt, data, temperature, max_tokens, model):\n",
    "    # data is already a list of lists (array type)\n",
    "    return ai_ask(prompt, data=data, temperature=temperature, max_tokens=max_tokens, model=model)\n",
    "\n",
    "demo_cases = [\n",
    "    [\n",
    "        \"Summarize the key findings from the employee engagement survey in 1 sentence:\",\n",
    "        [\n",
    "            [\"Question\", \"Score\"],\n",
    "            [\"Team collaboration\", 4.5],\n",
    "            [\"Workload\", 3.2],\n",
    "            [\"Career advancement\", 3.0],\n",
    "            [\"Management support\", 4.0]\n",
    "        ],\n",
    "        0.5,\n",
    "        250,\n",
    "        \"mistral-small-latest\"\n",
    "    ],\n",
    "    [\n",
    "        \"Provide a brief analysis of the quarterly sales performance in 1 sentence:\",\n",
    "        [\n",
    "            [\"Region\", \"Q1\", \"Q2\", \"Q3\", \"Q4\"],\n",
    "            [\"North\", 120, 135, 150, 160],\n",
    "            [\"South\", 100, 110, 120, 130],\n",
    "            [\"Central\", 90, 95, 100, 105]\n",
    "        ],\n",
    "        0.5,\n",
    "        250,\n",
    "        \"mistral-small-latest\"\n",
    "    ],\n",
    "    [\n",
    "        \"Summarize the following incident report in 1 sentence:\",\n",
    "        [\n",
    "            [\"On April 10th, a system outage affected order processing for 2 hours. The IT team resolved the issue by updating server configurations. No data loss occurred.\"]\n",
    "        ],\n",
    "        0.5,\n",
    "        250,\n",
    "        \"mistral-small-latest\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=ai_ask,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Prompt\", lines=2),\n",
    "        gr.Dataframe(label=\"Data (optional)\", type=\"array\", headers=None, datatype=\"str\", row_count=(1, \"dynamic\"), col_count=(1, \"dynamic\")),\n",
    "        gr.Slider(0.0, 2.0, value=0.5, step=0.01, label=\"Temperature\"),\n",
    "        gr.Number(value=250, label=\"Max Tokens\"),\n",
    "        gr.Textbox(value=\"mistral-small-latest\", label=\"Model\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"AI Response\"),\n",
    "    examples=demo_cases,\n",
    "    description=\"Interact with the AI Ask function. Enter a prompt and, optionally, provide tabular data for the AI to analyze or summarize. Adjust the temperature for creativity, set the maximum number of tokens for the response, and specify the model if desired.\",\n",
    "    flagging_mode=\"never\",\n",
    ")\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
